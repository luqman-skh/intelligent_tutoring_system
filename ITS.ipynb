{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install bitsandbytes transformers[accelerate] gradio"
      ],
      "metadata": {
        "id": "EHYLdQc3idfo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok huggingface_hub torch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qsM7zkkUeSvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken <token>"
      ],
      "metadata": {
        "id": "BSHbWotOeTgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from fastapi import FastAPI, Request\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "\n",
        "# Authenticate with Hugging Face using your access token\n",
        "login(\"<token>\")  # Replace with your actual token\n",
        "\n",
        "# Model configuration\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# 4-bit quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "\n",
        "# Chat function\n",
        "def chat_with_llama(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1000).to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=1000,\n",
        "        #max_new_tokens=1000,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "RUe8SlEoeVZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install firebase-admin"
      ],
      "metadata": {
        "id": "GzxCbGFdvRhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "\n",
        "# Initialize the Firebase Admin SDK\n",
        "cred = credentials.Certificate('/content/serviceAccountKey.json')\n",
        "firebase_admin.initialize_app(cred)"
      ],
      "metadata": {
        "id": "e2mWhRLYvUpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi[all]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KQUbWGzmeXU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request\n",
        "from pydantic import BaseModel\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uvicorn\n",
        "from pyngrok import ngrok  # For exposing the API to the internet\n",
        "import nest_asyncio\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import markdown\n",
        "import random\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "\n",
        "# Define weights for accuracy and improvement\n",
        "ALPHA = 0.7\n",
        "BETA = 0.3\n",
        "POPULATION_SIZE = 10  # Number of difficulty sets\n",
        "MUTATION_RATE = 0.2  # Probability of mutation\n",
        "QUESTION_COUNT = 3  # Only return 3 questions\n",
        "conversation_history: List[str] = []\n",
        "\n",
        "db = firestore.client()\n",
        "\n",
        "nest_asyncio.apply()\n",
        "# Define the FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "class UserRequest(BaseModel):\n",
        "    user_id: str\n",
        "\n",
        "\n",
        "# Add CORS middleware to allow requests from React (or other web apps)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # This allows requests from any origin. Change it for more security.\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],  # Allows all methods (GET, POST, etc.)\n",
        "    allow_headers=[\"*\"],  # Allows all headers\n",
        ")\n",
        "\n",
        "def calculate_improvement(current_accuracy, past_accuracies):\n",
        "    past_average = np.mean(past_accuracies) if past_accuracies else 0\n",
        "    return current_accuracy - past_average\n",
        "\n",
        "def generate_initial_population():\n",
        "    \"\"\"Generates initial difficulty sets for the population.\"\"\"\n",
        "    population = []\n",
        "    for _ in range(POPULATION_SIZE):\n",
        "        sequence = [random.choice([\"easy\", \"medium\", \"difficult\"]) for _ in range(QUESTION_COUNT)]\n",
        "        population.append(sequence)\n",
        "    return population\n",
        "\n",
        "def fitness_function(fitness_score, difficulty_set):\n",
        "    \"\"\"Evaluates how well a set of 3 difficulties matches the student.\"\"\"\n",
        "    difficulty_weights = {\"easy\": 1, \"medium\": 2, \"difficult\": 3}\n",
        "    total_difficulty = sum(difficulty_weights[d] for d in difficulty_set)\n",
        "\n",
        "    # Ideal difficulty should match fitness_score * 3 (scaled for 3 questions)\n",
        "    return -abs(total_difficulty - fitness_score * 3)\n",
        "\n",
        "def selection(population, fitness_scores):\n",
        "    \"\"\"Selects the best difficulty sets using tournament selection.\"\"\"\n",
        "    selected = []\n",
        "    while len(selected) < max(2, len(population) // 2):  # Ensure at least 2\n",
        "        i, j = random.sample(range(len(population)), 2)\n",
        "        selected.append(population[i] if fitness_scores[i] > fitness_scores[j] else population[j])\n",
        "    return selected\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "    \"\"\"Performs crossover to generate new difficulty assignments.\"\"\"\n",
        "    point = random.randint(1, len(parent1) - 1)\n",
        "    return parent1[:point] + parent2[point:], parent2[:point] + parent1[point:]\n",
        "\n",
        "def mutate(sequence):\n",
        "    \"\"\"Ensures mutation doesn't corrupt the sequence.\"\"\"\n",
        "    if random.random() < MUTATION_RATE:\n",
        "        index = random.randint(0, len(sequence) - 1)\n",
        "        new_value = random.choice([\"easy\", \"medium\", \"difficult\"])\n",
        "        sequence[index] = new_value if new_value != sequence[index] else sequence[index]  # Ensure change\n",
        "    return sequence\n",
        "\n",
        "def evolve_difficulty_assignment(fitness_score):\n",
        "    \"\"\"Uses evolutionary computing to generate an optimal difficulty assignment for 3 questions.\"\"\"\n",
        "    population = generate_initial_population()\n",
        "    print(f\"population: {population}\")\n",
        "\n",
        "    for _ in range(10):  # Number of generations\n",
        "        if not population:\n",
        "            raise ValueError(\"Population is empty. Check evolution logic.\")\n",
        "\n",
        "        fitness_scores = [fitness_function(fitness_score, seq) for seq in population]\n",
        "        selected = selection(population, fitness_scores)\n",
        "\n",
        "        # Ensure at least 2 individuals exist for crossover\n",
        "        if len(selected) < 2:\n",
        "            selected = generate_initial_population()[:2]\n",
        "\n",
        "        # Crossover and mutation to generate new population\n",
        "        new_population = []\n",
        "        for i in range(0, len(selected) - 1, 2):\n",
        "            child1, child2 = crossover(selected[i], selected[i + 1])\n",
        "            new_population.extend([mutate(child1), mutate(child2)])\n",
        "\n",
        "        # Ensure we always have a valid population\n",
        "        population = new_population if new_population else generate_initial_population()\n",
        "\n",
        "    print(f\"New population: {population}\")\n",
        "    # Ensure we have at least one valid sequence\n",
        "    if not population:\n",
        "        raise ValueError(\"Population is empty after evolution.\")\n",
        "\n",
        "    best_index = np.argmax([fitness_function(fitness_score, seq) for seq in population])\n",
        "    return population[best_index]\n",
        "\n",
        "\n",
        "def calculate_improvement(current_accuracy, past_accuracies):\n",
        "    # Calculate the average of past accuracies\n",
        "    past_average = np.mean(past_accuracies)\n",
        "    print(past_average)\n",
        "    return current_accuracy - past_average\n",
        "\n",
        "def getcontext(lessonId, subtopicId):\n",
        "    doc_ref = db.collection('topics').document(lessonId)\n",
        "    doc = doc_ref.get()\n",
        "    if doc.exists:\n",
        "        data = doc.to_dict()\n",
        "        # Navigate through the nested maps\n",
        "        subtopics = data.get('subtopics', {})  # Get the subtopics map\n",
        "        subtopic_data = subtopics.get(subtopicId, {})  # Get the specific subtopic\n",
        "        summary = subtopic_data.get('summary', 'No summary found')  # Get the summary field\n",
        "        print(\"Summary:\", summary)\n",
        "        return summary\n",
        "    else:\n",
        "        print(\"Document does not exist.\")\n",
        "\n",
        "def convert_to_count_format(difficulty_distribution):\n",
        "    difficulty_counts = {\"easy\": 0, \"medium\": 0, \"difficult\": 0}\n",
        "\n",
        "    for difficulty in difficulty_distribution:\n",
        "        difficulty_counts[difficulty] += 1\n",
        "\n",
        "    return difficulty_counts\n",
        "\n",
        "\n",
        "'''def filter_content(content):\n",
        "    prompt = f\"\"\"\n",
        "        Given the content remove unnecessary or repeated words or unnecessary html tags irrelevant of the context. Give only the clean ouput without any additonal texts as we will be directly sending it to the front-end:\n",
        "        HTML Content: {content}\n",
        "        AI:\n",
        "    \"\"\"\n",
        "    print(prompt)\n",
        "    response = chat_with_llama(prompt)\n",
        "    return response.split(\"AI:\")[-1].strip()'''\n",
        "\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def filter_context(context: str, query: str, num_chunks: int = 3) -> str:\n",
        "    \"\"\"Extract and filter text content from HTML using semantic headings structure\"\"\"\n",
        "    soup = BeautifulSoup(context, 'html.parser')\n",
        "    chunks = []\n",
        "\n",
        "    # Extract all headings and their content sections\n",
        "    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "\n",
        "    for heading in headings:\n",
        "        section_text = []\n",
        "        # Add heading text (strip HTML tags)\n",
        "        section_text.append(heading.get_text(separator=\" \", strip=True))\n",
        "\n",
        "        # Collect content until next heading\n",
        "        next_node = heading.next_sibling\n",
        "        while next_node and not is_heading(next_node):\n",
        "            if isinstance(next_node, Tag):\n",
        "                section_text.append(next_node.get_text(separator=\" \", strip=True))\n",
        "            next_node = next_node.next_sibling\n",
        "\n",
        "        chunks.append(\" \".join(section_text))\n",
        "\n",
        "    if not chunks:\n",
        "        return \"\"\n",
        "\n",
        "    # Semantic filtering with FAISS\n",
        "    embeddings = model.encode(chunks, convert_to_tensor=False)\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(np.array(embeddings).astype('float32'))\n",
        "\n",
        "    query_embedding = model.encode([query], convert_to_tensor=False)\n",
        "    distances, indices = index.search(\n",
        "        np.array(query_embedding).astype('float32'),\n",
        "        min(num_chunks, len(chunks))\n",
        "\n",
        "    return \"\\n\".join([chunks[i] for i in indices[0]])\n",
        "\n",
        "def is_heading(tag):\n",
        "    return isinstance(tag, Tag) and tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
        "\n",
        "# Request model for FastAPI\n",
        "class ChatRequest(BaseModel):\n",
        "    topic: str\n",
        "    context: Optional[str] = None\n",
        "    lessonId: Optional[str] = None\n",
        "    subtopicId: Optional[str] = None\n",
        "    #custom_prompt: str = None\n",
        "    custom_prompt: Optional[str] = None\n",
        "    options: Optional[List[str]] = None  # Add this for quiz options\n",
        "    correct_answer: Optional[str] = None  # Add this for the correct answer\n",
        "    selected_answer: Optional[str] = None  # Add this for the selected answer\n",
        "\n",
        "# Endpoint for quiz generation\n",
        "@app.post(\"/generate_quiz\")\n",
        "async def generate_quiz(request: ChatRequest):\n",
        "    prompt = f\"\"\"\n",
        "    You are tutoring a 12-year-old student.\n",
        "    Give 5 simple quiz questions each with 4 options for the following topic: {request.topic}\n",
        "    AI:\n",
        "    \"\"\"\n",
        "    response = chat_with_llama(prompt)\n",
        "    return {\"quiz\": response.split(\"AI:\")[1].strip()}\n",
        "\n",
        "@app.post(\"/reset_conversation\")\n",
        "async def reset_conversation():\n",
        "    global conversation_history\n",
        "    conversation_history = []\n",
        "\n",
        "\n",
        "# Endpoint for general chat\n",
        "@app.post(\"/ask_anything\")\n",
        "async def ask_anything(request: ChatRequest):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "        You are a helpful assistant. Respond to the following query and follow these rules:\n",
        "        1. Use HTML formatting for structure\n",
        "        2.restrict 18+ content\n",
        "        Query: {request.topic}\n",
        "        AI:\n",
        "    \"\"\"\n",
        "    response = chat_with_llama(prompt)\n",
        "    ai_response = response.split(\"AI:\")[1].strip()\n",
        "    clean_response = ai_response.split(\"Note:\")[0].strip()\n",
        "    #html_content = markdown.markdown(ai_response)\n",
        "    return {\"response\": clean_response}\n",
        "\n",
        "\n",
        "# Endpoint for general chat\n",
        "@app.post(\"/content_explanation\")\n",
        "async def content_explanation(request: ChatRequest):\n",
        "\n",
        "    global conversation_history\n",
        "    raw_context = getcontext(request.lessonId, request.subtopicId)\n",
        "    relevant_context = filter_context(raw_context, request.topic)\n",
        "    print(request.lessonId, request.subtopicId)\n",
        "    prompt = f\"\"\"\n",
        "        You are a helpful assistant tutoring a 12-year-old-student. Given the context and conversation that has been done already, respond to the following query and and format the respnose in html, add appropriate tags wherever necessary:\n",
        "        Context: {relevant_context}\n",
        "        Conversation: {conversation_history}\n",
        "        Query: {request.topic}\n",
        "        AI:\n",
        "    \"\"\"\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    response = chat_with_llama(prompt)\n",
        "    ai_response = response.split(\"AI:\")[-1].strip()\n",
        "    print(f\"Ai_response: {ai_response}\")\n",
        "    # Append new conversation history\n",
        "    conversation_history.append(f\"User: {request.topic}\")\n",
        "    conversation_history.append(f\"AI: {ai_response}\")\n",
        "    print(f\"Conversation History: {conversation_history}\")\n",
        "    #html_content = markdown.markdown(ai_response)\n",
        "    clean_response = ai_response.split(\"Note:\")[0].strip()\n",
        "    return {\"response\": clean_response}\n",
        "\n",
        "# Endpoint for general chat\n",
        "@app.post(\"/simplify\")\n",
        "async def simplify(request: ChatRequest):\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "        You are a helpful assistant tutoring a 12-year-old-student. Given the content from a textbook about a topic simplify it:\n",
        "        Textbook Content: {request.topic}\n",
        "        AI:\n",
        "    \"\"\"\n",
        "    print(prompt)\n",
        "    response = chat_with_llama(prompt)\n",
        "    print(response)\n",
        "\n",
        "    return {\"response\": response.split(\"AI:\")[-1].strip()}\n",
        "\n",
        "\n",
        "@app.post(\"/quiz_explanation\")\n",
        "async def quiz_explanation(request: ChatRequest):\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful assistant tutoring a 12-year-old student. Given the quiz questions with options, correct answer, and option selected by the user, provide a detailed feedback on why the selected answer is wrong and explain the correct answer (Format the response in html format, add appropriate tags wherever necessary):\n",
        "    Quiz question: {request.topic}\n",
        "    Options: {', '.join(request.options)}\n",
        "    Correct answer: {request.correct_answer}\n",
        "    Selected answer: {request.selected_answer}\n",
        "    AI:\n",
        "    \"\"\"\n",
        "    print(prompt)\n",
        "    response = chat_with_llama(prompt)\n",
        "    print(response)\n",
        "    ai_respnose = response.split(\"AI:\")[-1].strip()\n",
        "    clean_response = ai_response.split(\"Note:\")[0].strip()\n",
        "    return {\"response\": clean_response}\n",
        "    #return {\"response\": prompt}\n",
        "\n",
        "\n",
        "@app.post(\"/fitness_calculation\")\n",
        "async def fitness_calculation(user_request: dict):\n",
        "    user_id = user_request[\"user_id\"]\n",
        "    doc_ref = db.collection('users').document(user_id)\n",
        "\n",
        "    doc = doc_ref.get()\n",
        "    if not doc.exists:\n",
        "        raise HTTPException(status_code=404, detail=\"User document not found\")\n",
        "\n",
        "    data = doc.to_dict()\n",
        "    quiz_scores = [value['quizScore'] / 100 for key, value in data.get('progress', {}).items() if 'quizScore' in value]\n",
        "\n",
        "    if not quiz_scores:\n",
        "        raise HTTPException(status_code=400, detail=\"No quiz scores available\")\n",
        "\n",
        "    improvement = calculate_improvement(quiz_scores[-1], quiz_scores[:-1])\n",
        "    fitness_score = ALPHA * quiz_scores[-1] + BETA * improvement\n",
        "\n",
        "    # Use evolutionary computing to determine question difficulties for 3 questions\n",
        "    difficulty_distribution_old = evolve_difficulty_assignment(fitness_score)\n",
        "    difficulty_distribution = convert_to_count_format(difficulty_distribution_old)\n",
        "\n",
        "    return {\"difficulty_distribution\": difficulty_distribution}\n",
        "\n",
        "\n",
        "\n",
        "# Expose the API to the internet using ngrok\n",
        "def expose_ngrok():\n",
        "    url = ngrok.connect(8000)\n",
        "    print(f\"Public URL: {url}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Start ngrok\n",
        "    expose_ngrok()\n",
        "\n",
        "    # Run the FastAPI app\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "lymH_MW6gmXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6Rl7LaH1NFs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}